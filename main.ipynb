{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train\n",
    "from inference import inference\n",
    "from save_mask import save_mask\n",
    "import csv, os\n",
    "from utils.module import write_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"231026\"\n",
    "folnames = [project+\"_iter1\", project+\"_iter2\", project+\"_iter3\", project+\"_iter4\", project+\"_iter5\"]\n",
    "# os.makedirs('results/'+ project +'_results', exist_ok=True)\n",
    "csv_filename = 'results/'+project+'_results/results.csv'\n",
    "# with open(csv_filename, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(['Iteration', 'F1-Score', 'Accuracy', 'Specificity', 'Recall', 'Precision'])\n",
    "\n",
    "for i in range(len(folnames)):\n",
    "    if i==0 or i==1:\n",
    "        # train(former_folname=\"hoge\", folname=folnames[i], first=True, net=\"deeplab\", epochs=300, batch_size=64)\n",
    "        # scores = inference(former_folname=\"hoge\", folname=folnames[i], net=\"deeplab\", batch_size=64)\n",
    "        # write_to_csv(i+1, scores, csv_filename)\n",
    "        # save_mask(former_folname=\"hoge\", folname=folnames[i], net=\"deeplab\", batch_size=64, pred_max=True)\n",
    "        pass\n",
    "    elif i==2:\n",
    "        # train(former_folname=folnames[i-1], folname=folnames[i], first=False, net=\"deeplab\", epochs=300, batch_size=64, alpha=100)\n",
    "        scores = inference(former_folname=folnames[i-1], folname=folnames[i], net=\"deeplab\", batch_size=64)\n",
    "        write_to_csv(i+1, scores, csv_filename)\n",
    "        save_mask(former_folname=folnames[i-1], folname=folnames[i], net=\"deeplab\", batch_size=64, pred_max=True)\n",
    "    else:\n",
    "        train(former_folname=folnames[i-1], folname=folnames[i], first=False, net=\"deeplab\", epochs=300, batch_size=64, alpha=100)\n",
    "        scores = inference(former_folname=folnames[i-1], folname=folnames[i], net=\"deeplab\", batch_size=64)\n",
    "        write_to_csv(i+1, scores, csv_filename)\n",
    "        save_mask(former_folname=folnames[i-1], folname=folnames[i], net=\"deeplab\", batch_size=64, pred_max=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanlst = os.listdir(\"data/unlabeled_mask/231021_iter1/pred_mean_corrected/\")\n",
    "varlst = os.listdir(\"data/unlabeled_mask/231021_iter1/pred_var/\")\n",
    "print(len(meanlst))\n",
    "print(len(varlst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "meanlst = os.listdir(\"data/unlabeled_mask/231021_iter1/pred_mean_corrected/\")\n",
    "varlst = os.listdir(\"data/unlabeled_mask/231021_iter1/pred_var/\")\n",
    "\n",
    "# # 各画像を処理\n",
    "for file_name in meanlst:\n",
    "    file_path = os.path.join(\"data/unlabeled_mask/231021_iter1/pred_mean_corrected\", file_name)\n",
    "    \n",
    "    # 画像をnumpy配列に読み込む\n",
    "    img = np.array(Image.open(file_path))\n",
    "    \n",
    "    # 画像サイズが256x256でない場合、警告を表示\n",
    "    if img.shape != (256, 256):\n",
    "        print(f\"異なるサイズの画像: {file_name}, サイズ: {img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# フォルダのパス\n",
    "folder_path = \"data/unlabeled_mask/231021_iter1/pred_var/\"\n",
    "\n",
    "# フォルダ内の全ての.ptファイルをリストアップ\n",
    "pt_files = [f for f in os.listdir(folder_path) if f.endswith(\".pt\")]\n",
    "\n",
    "# 各ファイルを処理\n",
    "for file_name in pt_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # PyTorch tensorを読み込む\n",
    "    tensor = torch.load(file_path)\n",
    "    \n",
    "    # 配列サイズが(256, 256)でない場合、警告を表示\n",
    "    if tensor.size() != torch.Size([256, 256]):\n",
    "        print(f\"異なるサイズのPyTorch tensor: {file_name}, サイズ: {tensor.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "\n",
    "fol_name = \"231021_iter1\"\n",
    "\n",
    "# pred_varを全て読み込み、torch.meanとtorch.maxの平均を計算する\n",
    "pred_var_path = sorted(glob.glob(f'data/unlabeled_mask/{fol_name}/pred_var/*'))\n",
    "mean_box = []\n",
    "max_box = []\n",
    "\n",
    "for file in pred_var_path:\n",
    "    pred_var = torch.load(file)\n",
    "    mean_box.append(torch.mean(pred_var).item())\n",
    "    max_box.append(torch.max(pred_var).item())\n",
    "\n",
    "print(sum(mean_box)/len(mean_box))\n",
    "print(sum(max_box)/len(max_box))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
